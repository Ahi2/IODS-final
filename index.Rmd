---
title: "IODS-final"
author: "Mohamadali Ahi; mohamadali.ahi@helsinki.fi"
date: "15 joulukuuta 2017"
output:
  html_document:
    df_print: paged

---

In the assignment the follwing are going to be conducted: 

# hypotheses
The hypotheses are as follows:
H1. Gross National Income (GNP) per capita positively affects life expectancy at birth.

# 1. Multiple Linear Regression

I first begin with testing the first hypothesis. To do so, I need to conducta multiple linear analysis. 

# Reading the data into R

We first start with the first hypothesis. For that, we need to read the human dataset which has been prepared [here](https://github.com/Ahi2/IODS-final/blob/master/create_human2.R), using data wrangling procedure as explained in the link.

I first read the data again to make sure I am using the correct data. I call it 'human2'. Then, I look at the dtructure, dimensions, and a summary of the dataset.

```{r}
human2 <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", sep  =",", header = T)

str(human2)
dim(human2)
summary(human2)
```

Now, let us access the libraries we need.

```{r}
library(tidyr)

library(dplyr)

library(ggplot2)

library(GGally)

library(corrplot)

library(MASS)
```


# 1. Multiple Linear Regression 

```{r}
ggpairs(human2)
```

```{r}
cor(human2) %>% corrplot
```


```{r}
qplot(GNI, Life.Exp, data = human2) + geom_smooth(method = "lm")

```

Removing outliers. 
```{r}
human2 <- filter(human2, GNI < 80000)
```


```{r}
qplot(GNI, Life.Exp, data = human2) + geom_smooth(method = "lm")
```


```{r}
my_model <- lm(Life.Exp ~ GNI, data = human2) 
summary(my_model)
```


```{r}
my_model2 <- lm(Life.Exp ~ GNI + Mat.Mor, data = human2)
summary(my_model2)
```


```{r}
par(mfrow = c(2,2))
plot(my_model2, which = c(1,2,5))
```


# 2. Linear Discriminant Analysis (LDA)

LDA is a supervised classification method which can be used to model several binory vairbales. LDA is a linear transfomration technique used for dimensionality reduction. The independent variables are continuous while the target variables are categorical (binory). 

In this assignment, I am going to use different calsses of GNI as the target variable to see how other variables epxplain the classes. 

Because an assumption in LDA is that the variables are normally distributed, it is better to first standardize the data before performing the analysis. TO do so, I scale the whole dataset. 

# 2.1 Scaling Dataset  

```{r}
human2_scaled <- scale(human2)
summary(human2_scaled)
```

Now we look at the class of the scaled object, and convert the human2_scaled to a dataframe format.
```{r}
class(human2_scaled)
human2_scaled <- as.data.frame(human2_scaled)
```

# 2.2 Creating a categorical variable from GNI
Because I am interested in different classes of GNI explained by other variables, I now create categorical variables from GNI - which is currently a continous variable. I cut the variable by quantiles to get high, low, and middle rates of GNI per capita. Each quantile indicates the wealth of a country population. For example, high and low indicate wealthy and poor population, respectively.  

First, I look at the summary of GNI.

```{r}
summary(human2_scaled$GNI)
```

Now, I create a quantile vector of GNI and will print it. 

```{r}
bins <- quantile(human2_scaled$GNI)
bins
```

Now, I create the categorical variable, calling it GNI_class. Then, I will look at the table of the new factor GNI_class. 
```{r}
GNI_class <- cut(human2_scaled$GNI, breaks = bins, include.lowest = TRUE, c(label = "low", "med_low", "med_high", "high"))
table(GNI_class)
```

I now remove the original GNI from the dataset
```{r}
human2_scaled <- dplyr::select(human2_scaled, -GNI)
```

Then, I add the new categorical value, GNI_class, to scaled data.
```{r}
human2_scaled <- data.frame(human2_scaled, GNI_class)
```

Now, let's look at the summary of human2_scaled.

```{r}
summary(human2_scaled)
```

# 2.3 Dividing the data into train and test sets
In order to make prediction using a statistical method, it is important to test how well the predictions fit. To do so, one way is to split the data into test and train sets. As the names imply, the training of the model is done by the train set while the prediction is done using the test set. Let us do this. 

First, I look at the number of rows in the dataset, and save the number in 'n'.
```{r}
n <- nrow(human2_scaled)
n
```

I now choose randomly 80% of the rows.
```{r}
ind <- sample(n, size = n * 0.8)
```

Then, I create train set.
```{r}
train <- human2_scaled[ind,]
```

Then, the test set.
```{r}
test <- human2_scaled[-ind,]
```

Now, I tae the GNI classes from the test set and save them as 'correct_classes'.
```{r}
correct_classes <- test$GNI_class
```

Then, I remove the GNI_class from the test set.
```{r}
test <- dplyr::select(test, -GNI_class)
```

# 2.4 Performing Linear Disciminant Analysis
We have now created a catergorical variable of GNI, standardized the data, and splitted it into training and test datasets. We are ready to perform LDA. 
I now fit a linear discriminant analysis, using GNI_class as the target variable and all the other variables as predictors. Then, I will print it. 
```{r}
lda.fit <- lda(GNI_class ~ ., data = train)
lda.fit
```

The results above indicate that around 24% of the data belong to low GNI. The respective number for other GNI classes, med_low, med_high, and high are around 25%, 27%, and 23%, respectively. We then have the group means of the variables associated with each class.

We can also see that there are three linear discriminants (LD) becauase we have four taget classes. Proportion of trace shows that LD1 explains the majority of between-group variance (~ 88%).   

Looking at the coefficients of the analysis, we can infer that variables such as 'the ratio of Female and Male populations with secondary education' (Edu2.FM) predicts LD1 very well. The results indicate that the higher the values of this variable, the higher LD1. 

# 2.5 Visualizing LDA

LDA can be visualized with a biplot. There is a biplot arrow function [here](https://stackoverflow.com/questions/17232251/how-can-i-plot-a-biplot-for-lda-in-r), which I use to visualize the LDA.
```{r}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
```

To continue with plotting, I now create a numeric vector of the train sets crime classes.
```{r}
classes <- as.numeric(train$GNI_class)
```

Now, I plot the lda results. I use lda.arrow function to show the arrows, so the interpretation of the plot will be easier. 
```{r}
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 3)
```

In the above plot, each class has been shown by a distinct color. The plot demonstrates that, for instance, life expectancy (Life.Exp) is a significant variable that positively explains LD1 and also LD2. The ratio of females over males with at least secondary education (Edu2.FM) is also significant but negatively explains LD2, while positively explains LD1. 

# 2.6 Predicting LDA
Now that we have performed LDA, it is time to predict the values based on the model. We already created the test set to be used for predicting the model. The test set is a new dataset here, which can tell us how good the model fits our data. 

I now predict the classes with test data.
```{r}
lda.pred <- predict(lda.fit, newdata = test)
```

Then, I create a table of the correct classes and the predicted ones, running cross tabulation.
```{r}
table(correct = correct_classes, predicted = lda.pred$class)
```

The above results reveal that in most cases the GNI classes have been predicted correctly, particularly in low and high classes. There are some discrepancy in med_low and Med_high, but generally, we can conclude that, based on the data, the model predicts the classes well. Therefore, different classes of GNI - which divides countries into four categories based on their population income and hence standard of living - can be explained by variables in the model such as life expectancy and the ratio of female over male with secondary education.


